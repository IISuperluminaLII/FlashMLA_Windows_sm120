Build configuration:
FLASH_MLA_DISABLE_SM100=1
FLASH_MLA_DISABLE_SM90=1
FLASH_MLA_SM120_DISABLE_BWD=1

Excluding SM90 source files (disabled)
Excluding backward kernel (FLASH_MLA_SM120_DISABLE_BWD set)
SM100-specific decode/sparse files excluded (FLASH_MLA_DISABLE_SM100 set)
Training kernels (fwd/bwd): ALWAYS included (support SM100a + SM120)
Compiling using NVCC 12.9
Adding sm_120 (Blackwell workstation) support with PTX fallback
running clean
removing 'build\temp.win-amd64-cpython-312' (and everything under it)
removing 'build\lib.win-amd64-cpython-312' (and everything under it)
'build\bdist.win-amd64' does not exist -- can't clean it
'build\scripts-3.12' does not exist -- can't clean it
removing 'build'
Excluding SM90 source files (disabled)
Excluding backward kernel (FLASH_MLA_SM120_DISABLE_BWD set)
SM100-specific decode/sparse files excluded (FLASH_MLA_DISABLE_SM100 set)
Training kernels (fwd/bwd): ALWAYS included (support SM100a + SM120)
Compiling using NVCC 12.9
Adding sm_120 (Blackwell workstation) support with PTX fallback
running build_ext
W1029 20:47:11.940000 6064 site-packages\torch\utils\cpp_extension.py:480] Error checking compiler version for cl: [WinError 2] The system cannot find the file specified
W1029 20:47:11.947000 6064 site-packages\torch\utils\cpp_extension.py:521] The detected CUDA version (12.9) has a minor version mismatch with the version that was used to compile PyTorch (12.8). Most likely this shouldn't be a problem.
building 'flash_mla.cuda' extension
creating C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc
creating C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\sm100\prefill\dense
creating C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\smxx
W1029 20:47:13.871000 6064 site-packages\torch\utils\cpp_extension.py:480] Error checking compiler version for cl: [WinError 2] The system cannot find the file specified
C:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\setuptools\_distutils\_msvccompiler.py:12: UserWarning: _get_vc_env is private; find an alternative (pypa/distutils#340)
  warnings.warn(
[1/4] C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\bin\nvcc --generate-dependencies-with-compile --dependency-output C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\smxx\get_mla_metadata.obj.d -std=c++17 -Xcompiler /MD -Xcompiler /wd4819 -Xcompiler /wd4251 -Xcompiler /wd4244 -Xcompiler /wd4267 -Xcompiler /wd4275 -Xcompiler /wd4018 -Xcompiler /wd4190 -Xcompiler /wd4624 -Xcompiler /wd4067 -Xcompiler /wd4068 -Xcompiler /EHsc --use-local-env -Xcudafe --diag_suppress=base_class_has_different_dll_interface -Xcudafe --diag_suppress=field_without_dll_interface -Xcudafe --diag_suppress=dll_interface_conflict_none_assumed -Xcudafe --diag_suppress=dll_interface_conflict_dllexport_assumed -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm90 -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\tools\util\include "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\include\torch\csrc\api\include" "-IC:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Include" "-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.44.35207\include" "-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\VS\include" "-IC:\Program Files (x86)\Windows Kits\10\include\10.0.26100.0\ucrt" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\um" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\shared" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\winrt" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\cppwinrt" "-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um" -c C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\smxx\get_mla_metadata.cu -o C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\smxx\get_mla_metadata.obj -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -include msvc_compat.h -O3 -std=c++17 -DNDEBUG -D_USE_MATH_DEFINES -Wno-deprecated-declarations -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v,--register-usage-level=10 -Xcompiler /Zc:__cplusplus -Xcompiler /permissive- -Xcompiler /Zc:__cplusplus -DFLASH_MLA_DISABLE_SM90 -DFLASH_MLA_DISABLE_SM100 -DFLASH_MLA_SM120_DISABLE_BWD -gencode arch=compute_120,code=sm_120 -gencode arch=compute_120,code=compute_120 --threads 32 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=cuda
get_mla_metadata.cu
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_OPERATORS__' with '/U__CUDA_NO_HALF_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_CONVERSIONS__' with '/U__CUDA_NO_HALF_CONVERSIONS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF2_OPERATORS__' with '/U__CUDA_NO_HALF2_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_BFLOAT16_CONVERSIONS__' with '/U__CUDA_NO_BFLOAT16_CONVERSIONS__'
get_mla_metadata.cu
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_OPERATORS__' with '/U__CUDA_NO_HALF_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_CONVERSIONS__' with '/U__CUDA_NO_HALF_CONVERSIONS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF2_OPERATORS__' with '/U__CUDA_NO_HALF2_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_BFLOAT16_CONVERSIONS__' with '/U__CUDA_NO_BFLOAT16_CONVERSIONS__'
get_mla_metadata.cu
ptxas info    : 191 bytes gmem
ptxas info    : Compiling entry function '_Z23get_mla_metadata_kernel25GetDecodingMetadataParams' for 'sm_120'
ptxas info    : Function properties for _Z23get_mla_metadata_kernel25GetDecodingMetadataParams
    24 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, used 0 barriers, 24 bytes cumulative stack size
ptxas info    : Compile time = 0.000 ms
tmpxft_00001230_00000000-7_get_mla_metadata.cudafe1.cpp
[2/4] C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\bin\nvcc --generate-dependencies-with-compile --dependency-output C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\smxx\mla_combine.obj.d -std=c++17 -Xcompiler /MD -Xcompiler /wd4819 -Xcompiler /wd4251 -Xcompiler /wd4244 -Xcompiler /wd4267 -Xcompiler /wd4275 -Xcompiler /wd4018 -Xcompiler /wd4190 -Xcompiler /wd4624 -Xcompiler /wd4067 -Xcompiler /wd4068 -Xcompiler /EHsc --use-local-env -Xcudafe --diag_suppress=base_class_has_different_dll_interface -Xcudafe --diag_suppress=field_without_dll_interface -Xcudafe --diag_suppress=dll_interface_conflict_none_assumed -Xcudafe --diag_suppress=dll_interface_conflict_dllexport_assumed -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm90 -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\tools\util\include "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\include\torch\csrc\api\include" "-IC:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Include" "-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.44.35207\include" "-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\VS\include" "-IC:\Program Files (x86)\Windows Kits\10\include\10.0.26100.0\ucrt" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\um" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\shared" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\winrt" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\cppwinrt" "-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um" -c C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\smxx\mla_combine.cu -o C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\smxx\mla_combine.obj -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -include msvc_compat.h -O3 -std=c++17 -DNDEBUG -D_USE_MATH_DEFINES -Wno-deprecated-declarations -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v,--register-usage-level=10 -Xcompiler /Zc:__cplusplus -Xcompiler /permissive- -Xcompiler /Zc:__cplusplus -DFLASH_MLA_DISABLE_SM90 -DFLASH_MLA_DISABLE_SM100 -DFLASH_MLA_SM120_DISABLE_BWD -gencode arch=compute_120,code=sm_120 -gencode arch=compute_120,code=compute_120 --threads 32 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=cuda
mla_combine.cu
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_OPERATORS__' with '/U__CUDA_NO_HALF_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_CONVERSIONS__' with '/U__CUDA_NO_HALF_CONVERSIONS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF2_OPERATORS__' with '/U__CUDA_NO_HALF2_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_BFLOAT16_CONVERSIONS__' with '/U__CUDA_NO_BFLOAT16_CONVERSIONS__'
mla_combine.cu
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_OPERATORS__' with '/U__CUDA_NO_HALF_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_CONVERSIONS__' with '/U__CUDA_NO_HALF_CONVERSIONS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF2_OPERATORS__' with '/U__CUDA_NO_HALF2_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_BFLOAT16_CONVERSIONS__' with '/U__CUDA_NO_BFLOAT16_CONVERSIONS__'
mla_combine.cu
ptxas info    : 152 bytes gmem
ptxas info    : Compiling entry function '_Z28flash_fwd_mla_combine_kernelIN7cutlass6half_tELi512ELi8ELi64ELi256EEv14DecodingParams' for 'sm_120'
ptxas info    : Function properties for _Z28flash_fwd_mla_combine_kernelIN7cutlass6half_tELi512ELi8ELi64ELi256EEv14DecodingParams
    24 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, used 1 barriers, 24 bytes cumulative stack size
ptxas info    : Compile time = 0.000 ms
ptxas info    : Compiling entry function '_Z28flash_fwd_mla_combine_kernelIN7cutlass6half_tELi512ELi8ELi32ELi256EEv14DecodingParams' for 'sm_120'
ptxas info    : Function properties for _Z28flash_fwd_mla_combine_kernelIN7cutlass6half_tELi512ELi8ELi32ELi256EEv14DecodingParams
    24 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, used 1 barriers, 24 bytes cumulative stack size
ptxas info    : Compile time = 0.000 ms
ptxas info    : Compiling entry function '_Z28flash_fwd_mla_combine_kernelIN7cutlass10bfloat16_tELi512ELi8ELi64ELi256EEv14DecodingParams' for 'sm_120'
ptxas info    : Function properties for _Z28flash_fwd_mla_combine_kernelIN7cutlass10bfloat16_tELi512ELi8ELi64ELi256EEv14DecodingParams
    24 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, used 1 barriers, 24 bytes cumulative stack size
ptxas info    : Compile time = 0.000 ms
ptxas info    : Compiling entry function '_Z28flash_fwd_mla_combine_kernelIN7cutlass10bfloat16_tELi512ELi8ELi32ELi256EEv14DecodingParams' for 'sm_120'
ptxas info    : Function properties for _Z28flash_fwd_mla_combine_kernelIN7cutlass10bfloat16_tELi512ELi8ELi32ELi256EEv14DecodingParams
    24 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, used 1 barriers, 24 bytes cumulative stack size
ptxas info    : Compile time = 0.000 ms
tmpxft_00007a2c_00000000-7_mla_combine.cudafe1.cpp
[3/4] cl /showIncludes /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm90 -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\tools\util\include "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\include\torch\csrc\api\include" "-IC:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Include" "-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.44.35207\include" "-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\VS\include" "-IC:\Program Files (x86)\Windows Kits\10\include\10.0.26100.0\ucrt" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\um" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\shared" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\winrt" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\cppwinrt" "-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um" /MD /wd4819 /wd4251 /wd4244 /wd4267 /wd4275 /wd4018 /wd4190 /wd4624 /wd4067 /wd4068 /EHsc -c C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\pybind.cpp /FoC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\pybind.obj /O2 /std:c++17 /Zc:__cplusplus /EHsc /permissive- /DNOMINMAX /DWIN32_LEAN_AND_MEAN /D_HAS_EXCEPTIONS=1 /utf-8 /DNDEBUG /W0 /FImsvc_compat.h /DFLASH_MLA_DISABLE_SM90 /DFLASH_MLA_DISABLE_SM100 /DFLASH_MLA_SM120_DISABLE_BWD -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=cuda
cl : Command line warning D9025 : overriding '/W3' with '/W0'
[4/4] C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\bin\nvcc --generate-dependencies-with-compile --dependency-output C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.obj.d -std=c++17 -Xcompiler /MD -Xcompiler /wd4819 -Xcompiler /wd4251 -Xcompiler /wd4244 -Xcompiler /wd4267 -Xcompiler /wd4275 -Xcompiler /wd4018 -Xcompiler /wd4190 -Xcompiler /wd4624 -Xcompiler /wd4067 -Xcompiler /wd4068 -Xcompiler /EHsc --use-local-env -Xcudafe --diag_suppress=base_class_has_different_dll_interface -Xcudafe --diag_suppress=field_without_dll_interface -Xcudafe --diag_suppress=dll_interface_conflict_none_assumed -Xcudafe --diag_suppress=dll_interface_conflict_dllexport_assumed -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm90 -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\tools\util\include "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\include\torch\csrc\api\include" "-IC:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Include" "-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.44.35207\include" "-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\VS\include" "-IC:\Program Files (x86)\Windows Kits\10\include\10.0.26100.0\ucrt" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\um" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\shared" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\winrt" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\cppwinrt" "-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um" -c C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu -o C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.obj -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -include msvc_compat.h -O3 -std=c++17 -DNDEBUG -D_USE_MATH_DEFINES -Wno-deprecated-declarations -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v,--register-usage-level=10 -Xcompiler /Zc:__cplusplus -Xcompiler /permissive- -Xcompiler /Zc:__cplusplus -DFLASH_MLA_DISABLE_SM90 -DFLASH_MLA_DISABLE_SM100 -DFLASH_MLA_SM120_DISABLE_BWD -gencode arch=compute_120,code=sm_120 -gencode arch=compute_120,code=compute_120 --threads 32 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=cuda
FAILED: [code=4294967295] C:/PyCharmProjectsSpaceConflict/150BLLM/external/FlashMLA/build/temp.win-amd64-cpython-312/Release/csrc/sm100/prefill/dense/fmha_cutlass_fwd_sm100.obj 
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\bin\nvcc --generate-dependencies-with-compile --dependency-output C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.obj.d -std=c++17 -Xcompiler /MD -Xcompiler /wd4819 -Xcompiler /wd4251 -Xcompiler /wd4244 -Xcompiler /wd4267 -Xcompiler /wd4275 -Xcompiler /wd4018 -Xcompiler /wd4190 -Xcompiler /wd4624 -Xcompiler /wd4067 -Xcompiler /wd4068 -Xcompiler /EHsc --use-local-env -Xcudafe --diag_suppress=base_class_has_different_dll_interface -Xcudafe --diag_suppress=field_without_dll_interface -Xcudafe --diag_suppress=dll_interface_conflict_none_assumed -Xcudafe --diag_suppress=dll_interface_conflict_dllexport_assumed -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm90 -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\tools\util\include "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\include\torch\csrc\api\include" "-IC:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Include" "-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.44.35207\include" "-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\VS\include" "-IC:\Program Files (x86)\Windows Kits\10\include\10.0.26100.0\ucrt" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\um" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\shared" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\winrt" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\cppwinrt" "-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um" -c C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu -o C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.obj -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -include msvc_compat.h -O3 -std=c++17 -DNDEBUG -D_USE_MATH_DEFINES -Wno-deprecated-declarations -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v,--register-usage-level=10 -Xcompiler /Zc:__cplusplus -Xcompiler /permissive- -Xcompiler /Zc:__cplusplus -DFLASH_MLA_DISABLE_SM90 -DFLASH_MLA_DISABLE_SM100 -DFLASH_MLA_SM120_DISABLE_BWD -gencode arch=compute_120,code=sm_120 -gencode arch=compute_120,code=compute_120 --threads 32 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=cuda
fmha_cutlass_fwd_sm100.cu
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_OPERATORS__' with '/U__CUDA_NO_HALF_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_CONVERSIONS__' with '/U__CUDA_NO_HALF_CONVERSIONS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF2_OPERATORS__' with '/U__CUDA_NO_HALF2_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_BFLOAT16_CONVERSIONS__' with '/U__CUDA_NO_BFLOAT16_CONVERSIONS__'
fmha_cutlass_fwd_sm100.cu
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_OPERATORS__' with '/U__CUDA_NO_HALF_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_CONVERSIONS__' with '/U__CUDA_NO_HALF_CONVERSIONS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF2_OPERATORS__' with '/U__CUDA_NO_HALF2_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_BFLOAT16_CONVERSIONS__' with '/U__CUDA_NO_BFLOAT16_CONVERSIONS__'
fmha_cutlass_fwd_sm100.cu
C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/fmha_fusion.hpp(129): warning #221-D: floating-point value does not fit in required floating-point type
          acc_qk(i) = -((float)(1e+300));
                        ^

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/fmha_fusion.hpp(181): warning #221-D: floating-point value does not fit in required floating-point type
          acc_qk(i) = -((float)(1e+300));
                        ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/fmha_fusion.hpp(263): warning #221-D: floating-point value does not fit in required floating-point type
            acc_qk(i) = -((float)(1e+300));
                          ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/fmha_fusion.hpp(272): warning #221-D: floating-point value does not fit in required floating-point type
            acc_qk(i) = -((float)(1e+300));
                          ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/fmha_fusion.hpp(309): warning #221-D: floating-point value does not fit in required floating-point type
          acc_qk(i) = -((float)(1e+300));
                        ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/sm100_fmha_fwd_mainloop_tma_warpspecialized.hpp(594): warning #221-D: floating-point value does not fit in required floating-point type
      ElementQK row_max_safe = row_max == -((float)(1e+300)) ? 0 : row_max;
                                            ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/sm100_fmha_fwd_mainloop_tma_warpspecialized.hpp(725): warning #221-D: floating-point value does not fit in required floating-point type
      ElementQK row_max = -((float)(1e+300));
                            ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/sm100_fmha_fwd_mainloop_tma_warpspecialized.hpp(1155): warning #221-D: floating-point value does not fit in required floating-point type
      float lse = -((float)(1e+300));
                    ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/sm100_fmha_mla_fwd_mainloop_tma_warpspecialized.hpp(618): warning #221-D: floating-point value does not fit in required floating-point type
      ElementQK row_max_safe = row_max == -((float)(1e+300)) ? 0 : row_max;
                                            ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/sm100_fmha_mla_fwd_mainloop_tma_warpspecialized.hpp(750): warning #221-D: floating-point value does not fit in required floating-point type
      ElementQK row_max = -((float)(1e+300));
                            ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/sm100_fmha_mla_fwd_mainloop_tma_warpspecialized.hpp(1165): warning #221-D: floating-point value does not fit in required floating-point type
      float lse = -((float)(1e+300));
                    ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/sm120_fmha_universal_fwd_mainloop.hpp(282): warning #221-D: floating-point value does not fit in required floating-point type
        ElementAcc max_val = -((float)(1e+300));
                               ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/sm120_fmha_universal_fwd_mainloop.hpp(286): warning #221-D: floating-point value does not fit in required floating-point type
            sP(m, n) = -((float)(1e+300));
                         ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/sm120_fmha_universal_fwd_mainloop.hpp(418): warning #221-D: floating-point value does not fit in required floating-point type
        sLSE(m) = -((float)(1e+300));
                    ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(228): warning #550-D: variable "problem_size" was set but never used
      decltype(problem_shape_in) problem_size;
                                 ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(240): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/sm120_fmha_universal_fwd_mainloop.hpp(68): error: no suitable conversion function from "std::conditional_t<true, cute::tuple_element_t<2ULL, cute::tuple<cute::_64, cute::_32, flash::Sm120WorkstationConfig::HeadDim>>, cute::tuple_element_t<2ULL, cute::tuple<cute::_64, cute::_32, flash::Sm120WorkstationConfig::HeadDim>> &&>" (aka "std::conditional_t<true, cute::tuple<cute::C<64>, cute::C<64>>, cute::tuple<cute::C<64>, cute::C<64>> &&>") to "const int" exists
    static constexpr int kHeadDim = get<2>(TileShape{});  
                                    ^
          detected during:
            instantiation of class "flash::Sm120FmhaUniversalFwdMainloop<KernelTraits_, ProblemShape_, Element_, ElementAcc_, TileShape_> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShape_=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, Element_=cutlass::bfloat16_t, ElementAcc_=float, TileShape_=flash::Sm120WorkstationConfig::TileShapeMlaFwd]" at line 140 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 60 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(166): error: class "flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>" has no member "ClusterShape"
    using ClusterShape = typename CollectiveMainloop::ClusterShape;
                                                      ^
          detected during:
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 60 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(172): error: class "flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>" has no member "TensorStorage"
        typename CollectiveMainloop::TensorStorage mainloop;
                                     ^
          detected during:
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 205
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 60 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(177): error: class "flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>" has no member "TensorStorage"
        typename CollectiveMainloop::TensorStorage mainloop;
                                     ^
          detected during:
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 205
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 60 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(184): error: class "flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>" has no member "IsOrderLoadEpilogue"
                                                                            std::conditional_t<CollectiveMainloop::IsOrderLoadEpilogue, UnionType, StructType>,
                                                                                                                   ^
          detected during:
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 205
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 60 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(191): error: name followed by "::" must be a class or namespace name
        alignas(16) typename CollectiveMainloop::PipelineQ::SharedStorage load_q;
                                                 ^
          detected during:
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage::PipelineStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 200
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 205
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 60 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(192): error: name followed by "::" must be a class or namespace name
        alignas(16) typename CollectiveMainloop::PipelineKV::SharedStorage load_kv;
                                                 ^
          detected during:
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage::PipelineStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 200
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 205
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 60 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(193): error: name followed by "::" must be a class or namespace name
        alignas(16) typename CollectiveMainloop::PipelineS::SharedStorage mma_s0;
                                                 ^
          detected during:
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage::PipelineStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 200
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 205
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 60 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(194): error: name followed by "::" must be a class or namespace name
        alignas(16) typename CollectiveMainloop::PipelineS::SharedStorage mma_s1;
                                                 ^
          detected during:
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage::PipelineStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 200
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 205
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 60 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(195): error: name followed by "::" must be a class or namespace name
        alignas(16) typename CollectiveMainloop::PipelineC::SharedStorage s0_corr;
                                                 ^
          detected during:
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage::PipelineStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 200
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 205
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 60 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(196): error: name followed by "::" must be a class or namespace name
        alignas(16) typename CollectiveMainloop::PipelineC::SharedStorage s1_corr;
                                                 ^
          detected during:
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage::PipelineStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 200
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 205
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 60 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(197): error: name followed by "::" must be a class or namespace name
        alignas(16) typename CollectiveMainloop::PipelineO::SharedStorage mma_corr;
                                                 ^
          detected during:
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage::PipelineStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 200
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 205
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 60 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(198): error: name followed by "::" must be a class or namespace name
        alignas(16) typename CollectiveMainloop::PipelineE::SharedStorage corr_epi;
                                                 ^
          detected during:
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage::PipelineStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 200
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 205
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 60 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(199): error: name followed by "::" must be a class or namespace name
        alignas(16) typename CollectiveMainloop::OrderBarrierSoftmax::SharedStorage order_s01;
                                                 ^
          detected during:
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage::PipelineStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 200
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 205
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 60 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(230): error: class "flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>" has no member "Params"
      typename CollectiveMainloop::Params mainloop;
                                   ^
          detected during:
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::Params [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 70 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(223): error: class "flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>" has no member "Arguments"
      typename CollectiveMainloop::Arguments mainloop;
                                   ^
          detected during:
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::Arguments [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(247): error: class "flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>" has no member "can_implement"
      return CollectiveMainloop::can_implement(args.problem_shape, args.mainloop);
                                 ^
          detected during:
            instantiation of "__nv_bool cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::can_implement(const cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::Arguments &) [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 88 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of "cutlass::Status cutlass::fmha::device::FMHA<Kernel_>::can_implement(const cutlass::fmha::device::FMHA<Kernel_>::Arguments &) [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 333 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(262): error: class "flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>" has no member "to_underlying_arguments"
          CollectiveMainloop::to_underlying_arguments(args.problem_shape, args.mainloop, workspace),
                              ^
          detected during:
            instantiation of "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::Params cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::to_underlying_arguments(const cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::Arguments &, void *) [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 165 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of "cutlass::Status cutlass::fmha::device::FMHA<Kernel_>::initialize(const cutlass::fmha::device::FMHA<Kernel_>::Arguments &, void *, cudaStream_t) [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 334 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(228): warning #550-D: variable "problem_size" was set but never used
      decltype(problem_shape_in) problem_size;
                                 ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 383
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(240): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 383
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(166): error: class "flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>" has no member "ClusterShape"
    using ClusterShape = typename CollectiveMainloop::ClusterShape;
                                                      ^
          detected during:
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::IndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 60 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::IndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 383 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(184): error: class "flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>" has no member "IsOrderLoadEpilogue"
                                                                            std::conditional_t<CollectiveMainloop::IsOrderLoadEpilogue, UnionType, StructType>,
                                                                                                                   ^
          detected during:
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::IndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 205
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::IndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 60 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::IndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 383 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(247): error: class "flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>" has no member "can_implement"
      return CollectiveMainloop::can_implement(args.problem_shape, args.mainloop);
                                 ^
          detected during:
            instantiation of "__nv_bool cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::can_implement(const cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::Arguments &) [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::IndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 88 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of "cutlass::Status cutlass::fmha::device::FMHA<Kernel_>::can_implement(const cutlass::fmha::device::FMHA<Kernel_>::Arguments &) [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::IndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 333 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 383 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(262): error: class "flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>" has no member "to_underlying_arguments"
          CollectiveMainloop::to_underlying_arguments(args.problem_shape, args.mainloop, workspace),
                              ^
          detected during:
            instantiation of "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::Params cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::to_underlying_arguments(const cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::Arguments &, void *) [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::IndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 165 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of "cutlass::Status cutlass::fmha::device::FMHA<Kernel_>::initialize(const cutlass::fmha::device::FMHA<Kernel_>::Arguments &, void *, cudaStream_t) [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<cutlass::fmha::collective::VariableLength, cutlass::fmha::collective::VariableLength, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::IndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 334 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 383 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu(24): warning #177-D: variable "IsCausalMask" was declared but never referenced
    static constexpr bool IsCausalMask = std::is_same_v<Mask, CausalMask<false>>;
                          ^
          detected during instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(240): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 379
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=false, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::false_type]" at line 136 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(240): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 383
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=false, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::false_type]" at line 136 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu(24): warning #177-D: variable "IsCausalMask" was declared but never referenced
    static constexpr bool IsCausalMask = std::is_same_v<Mask, CausalMask<false>>;
                          ^
          detected during instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::false_type]" at line 136

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(228): warning #550-D: variable "problem_size" was set but never used
      decltype(problem_shape_in) problem_size;
                                 ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(240): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/sm120_fmha_universal_fwd_mainloop.hpp(68): error: no suitable conversion function from "std::conditional_t<true, cute::tuple_element_t<2ULL, cute::tuple<cute::_64, cute::_32, flash::Sm120WorkstationConfig::HeadDim>>, cute::tuple_element_t<2ULL, cute::tuple<cute::_64, cute::_32, flash::Sm120WorkstationConfig::HeadDim>> &&>" (aka "std::conditional_t<true, cute::tuple<cute::C<64>, cute::C<64>>, cute::tuple<cute::C<64>, cute::C<64>> &&>") to "const int" exists
    static constexpr int kHeadDim = get<2>(TileShape{});  
                                    ^
          detected during:
            instantiation of class "flash::Sm120FmhaUniversalFwdMainloop<KernelTraits_, ProblemShape_, Element_, ElementAcc_, TileShape_> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShape_=cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, Element_=cutlass::bfloat16_t, ElementAcc_=float, TileShape_=flash::Sm120WorkstationConfig::TileShapeMlaFwd]" at line 140 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 60 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(166): error: class "flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>" has no member "ClusterShape"
    using ClusterShape = typename CollectiveMainloop::ClusterShape;
                                                      ^
          detected during:
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 60 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(184): error: class "flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>" has no member "IsOrderLoadEpilogue"
                                                                            std::conditional_t<CollectiveMainloop::IsOrderLoadEpilogue, UnionType, StructType>,
                                                                                                                   ^
          detected during:
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 205
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 60 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(247): error: class "flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>" has no member "can_implement"
      return CollectiveMainloop::can_implement(args.problem_shape, args.mainloop);
                                 ^
          detected during:
            instantiation of "__nv_bool cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::can_implement(const cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::Arguments &) [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 88 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of "cutlass::Status cutlass::fmha::device::FMHA<Kernel_>::can_implement(const cutlass::fmha::device::FMHA<Kernel_>::Arguments &) [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 333 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(262): error: class "flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>" has no member "to_underlying_arguments"
          CollectiveMainloop::to_underlying_arguments(args.problem_shape, args.mainloop, workspace),
                              ^
          detected during:
            instantiation of "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::Params cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::to_underlying_arguments(const cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::Arguments &, void *) [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::CausalIndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 165 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of "cutlass::Status cutlass::fmha::device::FMHA<Kernel_>::initialize(const cutlass::fmha::device::FMHA<Kernel_>::Arguments &, void *, cudaStream_t) [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::CausalIndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 334 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(228): warning #550-D: variable "problem_size" was set but never used
      decltype(problem_shape_in) problem_size;
                                 ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 383
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(240): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 383
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(166): error: class "flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>" has no member "ClusterShape"
    using ClusterShape = typename CollectiveMainloop::ClusterShape;
                                                      ^
          detected during:
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::IndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 60 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::IndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 383 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(184): error: class "flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>" has no member "IsOrderLoadEpilogue"
                                                                            std::conditional_t<CollectiveMainloop::IsOrderLoadEpilogue, UnionType, StructType>,
                                                                                                                   ^
          detected during:
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::SharedStorage [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::IndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 205
            instantiation of class "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule> [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::IndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 60 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of class "cutlass::fmha::device::FMHA<Kernel_> [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::IndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 320 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 383 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(247): error: class "flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>" has no member "can_implement"
      return CollectiveMainloop::can_implement(args.problem_shape, args.mainloop);
                                 ^
          detected during:
            instantiation of "__nv_bool cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::can_implement(const cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::Arguments &) [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::IndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 88 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of "cutlass::Status cutlass::fmha::device::FMHA<Kernel_>::can_implement(const cutlass::fmha::device::FMHA<Kernel_>::Arguments &) [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::IndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 333 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 383 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\kernel/sm100_fmha_fwd_kernel_tma_warpspecialized.hpp(262): error: class "flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>" has no member "to_underlying_arguments"
          CollectiveMainloop::to_underlying_arguments(args.problem_shape, args.mainloop, workspace),
                              ^
          detected during:
            instantiation of "cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::Params cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::to_underlying_arguments(const cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<KernelTraits_, ProblemShapeIn, CollectiveMainloop, CollectiveEpilogue, TileScheduler, KernelSchedule>::Arguments &, void *) [with KernelTraits_=flash::Sm120WorkstationConfig, ProblemShapeIn=cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, CollectiveMainloop=flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, CollectiveEpilogue=cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, TileScheduler=cutlass::fmha::kernel::IndividualTileScheduler, KernelSchedule=cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule]" at line 165 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\device/fmha.hpp
            instantiation of "cutlass::Status cutlass::fmha::device::FMHA<Kernel_>::initialize(const cutlass::fmha::device::FMHA<Kernel_>::Arguments &, void *, cudaStream_t) [with Kernel_=cutlass::fmha::kernel::Sm100FmhaFwdKernelTmaWarpspecialized<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, flash::Sm120FmhaUniversalFwdMainloop<flash::Sm120WorkstationConfig, cute::tuple<int, int, cute::tuple<int32_t, int32_t>, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::TileShapeMlaFwd>, cutlass::fmha::collective::Sm100FmhaFwdEpilogueTmaWarpspecialized<cutlass::bfloat16_t, float, flash::Sm120WorkstationConfig::EpilogueTileShape, cute::tuple<int, cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::tuple<cute::_1, cute::tuple<cute::tuple<int32_t, int32_t>, int>>, cute::false_type>, cutlass::fmha::kernel::IndividualTileScheduler, cutlass::fmha::kernel::Sm100MlaFwdCtxKernelWarpspecializedSchedule>]" at line 334 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 383 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu(24): warning #177-D: variable "IsCausalMask" was declared but never referenced
    static constexpr bool IsCausalMask = std::is_same_v<Mask, CausalMask<false>>;
                          ^
          detected during instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(240): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 379
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=false, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::false_type]" at line 136 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(240): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 383
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=false, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::false_type]" at line 136 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu(24): warning #177-D: variable "IsCausalMask" was declared but never referenced
    static constexpr bool IsCausalMask = std::is_same_v<Mask, CausalMask<false>>;
                          ^
          detected during instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::false_type]" at line 136

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(228): warning #550-D: variable "problem_size" was set but never used
      decltype(problem_shape_in) problem_size;
                                 ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(240): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(228): warning #550-D: variable "problem_size" was set but never used
      decltype(problem_shape_in) problem_size;
                                 ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 383
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(240): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 383
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu(24): warning #177-D: variable "IsCausalMask" was declared but never referenced
    static constexpr bool IsCausalMask = std::is_same_v<Mask, CausalMask<false>>;
                          ^
          detected during instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(240): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 379
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=false, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::false_type]" at line 136 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(240): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 383
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=false, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::false_type]" at line 136 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu(24): warning #177-D: variable "IsCausalMask" was declared but never referenced
    static constexpr bool IsCausalMask = std::is_same_v<Mask, CausalMask<false>>;
                          ^
          detected during instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::false_type]" at line 136

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(228): warning #550-D: variable "problem_size" was set but never used
      decltype(problem_shape_in) problem_size;
                                 ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(240): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 379
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(228): warning #550-D: variable "problem_size" was set but never used
      decltype(problem_shape_in) problem_size;
                                 ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 383
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(240): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 383
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu(24): warning #177-D: variable "IsCausalMask" was declared but never referenced
    static constexpr bool IsCausalMask = std::is_same_v<Mask, CausalMask<false>>;
                          ^
          detected during instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 132

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(240): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 379
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=false, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::false_type]" at line 136 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(240): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 286
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 383
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=false, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 76 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::false_type]" at line 136 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu(24): warning #177-D: variable "IsCausalMask" was declared but never referenced
    static constexpr bool IsCausalMask = std::is_same_v<Mask, CausalMask<false>>;
                          ^
          detected during instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::false_type]" at line 136

31 errors detected in the compilation of "C:/PyCharmProjectsSpaceConflict/150BLLM/external/FlashMLA/csrc/sm100/prefill/dense/fmha_cutlass_fwd_sm100.cu".
ninja: build stopped: subcommand failed.
Traceback (most recent call last):
  File "C:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\utils\cpp_extension.py", line 2597, in _run_ninja_build
    subprocess.run(
  File "C:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 4294967295.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\setup.py", line 192, in <module>
    setup(
  File "C:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\setuptools\__init__.py", line 115, in setup
    return distutils.core.setup(**attrs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\setuptools\_distutils\core.py", line 186, in setup
    return run_commands(dist)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\setuptools\_distutils\core.py", line 202, in run_commands
    dist.run_commands()
  File "C:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\setuptools\_distutils\dist.py", line 1002, in run_commands
    self.run_command(cmd)
  File "C:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\setuptools\dist.py", line 1102, in run_command
    super().run_command(command)
  File "C:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\setuptools\_distutils\dist.py", line 1021, in run_command
    cmd_obj.run()
  File "C:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\setuptools\command\build_ext.py", line 96, in run
    _build_ext.run(self)
  File "C:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\setuptools\_distutils\command\build_ext.py", line 368, in run
    self.build_extensions()
  File "C:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\utils\cpp_extension.py", line 1082, in build_extensions
    build_ext.build_extensions(self)
  File "C:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\setuptools\_distutils\command\build_ext.py", line 484, in build_extensions
    self._build_extensions_serial()
  File "C:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\setuptools\_distutils\command\build_ext.py", line 510, in _build_extensions_serial
    self.build_extension(ext)
  File "C:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\setuptools\command\build_ext.py", line 261, in build_extension
    _build_ext.build_extension(self, ext)
  File "C:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\setuptools\_distutils\command\build_ext.py", line 565, in build_extension
    objects = self.compiler.compile(
              ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\utils\cpp_extension.py", line 1051, in win_wrap_ninja_compile
    _write_ninja_file_and_compile_objects(
  File "C:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\utils\cpp_extension.py", line 2223, in _write_ninja_file_and_compile_objects
    _run_ninja_build(
  File "C:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\utils\cpp_extension.py", line 2614, in _run_ninja_build
    raise RuntimeError(message) from e
RuntimeError: Error compiling objects for extension
