Build configuration:
FLASH_MLA_DISABLE_SM100=1
FLASH_MLA_DISABLE_SM90=1
FLASH_MLA_SM120_DISABLE_BWD=1

Excluding SM90 source files (disabled)
Excluding backward kernel (FLASH_MLA_SM120_DISABLE_BWD set)
SM100-specific decode/sparse files excluded (FLASH_MLA_DISABLE_SM100 set)
Training kernels (fwd/bwd): ALWAYS included (support SM100a + SM120)
Compiling using NVCC 12.9
Adding sm_120 (Blackwell workstation) support with PTX fallback
running clean
removing 'build\temp.win-amd64-cpython-312' (and everything under it)
'build\lib.win-amd64-cpython-312' does not exist -- can't clean it
'build\bdist.win-amd64' does not exist -- can't clean it
'build\scripts-3.12' does not exist -- can't clean it
removing 'build'
Excluding SM90 source files (disabled)
Excluding backward kernel (FLASH_MLA_SM120_DISABLE_BWD set)
SM100-specific decode/sparse files excluded (FLASH_MLA_DISABLE_SM100 set)
Training kernels (fwd/bwd): ALWAYS included (support SM100a + SM120)
Compiling using NVCC 12.9
Adding sm_120 (Blackwell workstation) support with PTX fallback
running build_ext
W1029 20:49:55.712000 19516 site-packages\torch\utils\cpp_extension.py:480] Error checking compiler version for cl: [WinError 2] The system cannot find the file specified
W1029 20:49:55.720000 19516 site-packages\torch\utils\cpp_extension.py:521] The detected CUDA version (12.9) has a minor version mismatch with the version that was used to compile PyTorch (12.8). Most likely this shouldn't be a problem.
building 'flash_mla.cuda' extension
creating C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc
creating C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\sm100\prefill\dense
creating C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\smxx
W1029 20:49:57.688000 19516 site-packages\torch\utils\cpp_extension.py:480] Error checking compiler version for cl: [WinError 2] The system cannot find the file specified
C:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\setuptools\_distutils\_msvccompiler.py:12: UserWarning: _get_vc_env is private; find an alternative (pypa/distutils#340)
  warnings.warn(
[1/4] C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\bin\nvcc --generate-dependencies-with-compile --dependency-output C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\smxx\get_mla_metadata.obj.d -std=c++17 -Xcompiler /MD -Xcompiler /wd4819 -Xcompiler /wd4251 -Xcompiler /wd4244 -Xcompiler /wd4267 -Xcompiler /wd4275 -Xcompiler /wd4018 -Xcompiler /wd4190 -Xcompiler /wd4624 -Xcompiler /wd4067 -Xcompiler /wd4068 -Xcompiler /EHsc --use-local-env -Xcudafe --diag_suppress=base_class_has_different_dll_interface -Xcudafe --diag_suppress=field_without_dll_interface -Xcudafe --diag_suppress=dll_interface_conflict_none_assumed -Xcudafe --diag_suppress=dll_interface_conflict_dllexport_assumed -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm90 -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\tools\util\include "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\include\torch\csrc\api\include" "-IC:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Include" "-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.44.35207\include" "-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\VS\include" "-IC:\Program Files (x86)\Windows Kits\10\include\10.0.26100.0\ucrt" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\um" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\shared" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\winrt" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\cppwinrt" "-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um" -c C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\smxx\get_mla_metadata.cu -o C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\smxx\get_mla_metadata.obj -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -include msvc_compat.h -O3 -std=c++17 -DNDEBUG -D_USE_MATH_DEFINES -Wno-deprecated-declarations -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v,--register-usage-level=10 -Xcompiler /Zc:__cplusplus -Xcompiler /permissive- -Xcompiler /Zc:__cplusplus -DFLASH_MLA_DISABLE_SM90 -DFLASH_MLA_DISABLE_SM100 -DFLASH_MLA_SM120_DISABLE_BWD -gencode arch=compute_120,code=sm_120 -gencode arch=compute_120,code=compute_120 --threads 32 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=cuda
get_mla_metadata.cu
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_OPERATORS__' with '/U__CUDA_NO_HALF_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_CONVERSIONS__' with '/U__CUDA_NO_HALF_CONVERSIONS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF2_OPERATORS__' with '/U__CUDA_NO_HALF2_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_BFLOAT16_CONVERSIONS__' with '/U__CUDA_NO_BFLOAT16_CONVERSIONS__'
get_mla_metadata.cu
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_OPERATORS__' with '/U__CUDA_NO_HALF_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_CONVERSIONS__' with '/U__CUDA_NO_HALF_CONVERSIONS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF2_OPERATORS__' with '/U__CUDA_NO_HALF2_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_BFLOAT16_CONVERSIONS__' with '/U__CUDA_NO_BFLOAT16_CONVERSIONS__'
get_mla_metadata.cu
ptxas info    : 191 bytes gmem
ptxas info    : Compiling entry function '_Z23get_mla_metadata_kernel25GetDecodingMetadataParams' for 'sm_120'
ptxas info    : Function properties for _Z23get_mla_metadata_kernel25GetDecodingMetadataParams
    24 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, used 0 barriers, 24 bytes cumulative stack size
ptxas info    : Compile time = 0.000 ms
tmpxft_000075b0_00000000-7_get_mla_metadata.cudafe1.cpp
[2/4] C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\bin\nvcc --generate-dependencies-with-compile --dependency-output C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\smxx\mla_combine.obj.d -std=c++17 -Xcompiler /MD -Xcompiler /wd4819 -Xcompiler /wd4251 -Xcompiler /wd4244 -Xcompiler /wd4267 -Xcompiler /wd4275 -Xcompiler /wd4018 -Xcompiler /wd4190 -Xcompiler /wd4624 -Xcompiler /wd4067 -Xcompiler /wd4068 -Xcompiler /EHsc --use-local-env -Xcudafe --diag_suppress=base_class_has_different_dll_interface -Xcudafe --diag_suppress=field_without_dll_interface -Xcudafe --diag_suppress=dll_interface_conflict_none_assumed -Xcudafe --diag_suppress=dll_interface_conflict_dllexport_assumed -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm90 -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\tools\util\include "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\include\torch\csrc\api\include" "-IC:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Include" "-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.44.35207\include" "-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\VS\include" "-IC:\Program Files (x86)\Windows Kits\10\include\10.0.26100.0\ucrt" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\um" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\shared" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\winrt" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\cppwinrt" "-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um" -c C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\smxx\mla_combine.cu -o C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\smxx\mla_combine.obj -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -include msvc_compat.h -O3 -std=c++17 -DNDEBUG -D_USE_MATH_DEFINES -Wno-deprecated-declarations -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v,--register-usage-level=10 -Xcompiler /Zc:__cplusplus -Xcompiler /permissive- -Xcompiler /Zc:__cplusplus -DFLASH_MLA_DISABLE_SM90 -DFLASH_MLA_DISABLE_SM100 -DFLASH_MLA_SM120_DISABLE_BWD -gencode arch=compute_120,code=sm_120 -gencode arch=compute_120,code=compute_120 --threads 32 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=cuda
mla_combine.cu
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_OPERATORS__' with '/U__CUDA_NO_HALF_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_CONVERSIONS__' with '/U__CUDA_NO_HALF_CONVERSIONS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF2_OPERATORS__' with '/U__CUDA_NO_HALF2_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_BFLOAT16_CONVERSIONS__' with '/U__CUDA_NO_BFLOAT16_CONVERSIONS__'
mla_combine.cu
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_OPERATORS__' with '/U__CUDA_NO_HALF_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_CONVERSIONS__' with '/U__CUDA_NO_HALF_CONVERSIONS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF2_OPERATORS__' with '/U__CUDA_NO_HALF2_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_BFLOAT16_CONVERSIONS__' with '/U__CUDA_NO_BFLOAT16_CONVERSIONS__'
mla_combine.cu
ptxas info    : 152 bytes gmem
ptxas info    : Compiling entry function '_Z28flash_fwd_mla_combine_kernelIN7cutlass6half_tELi512ELi8ELi64ELi256EEv14DecodingParams' for 'sm_120'
ptxas info    : Function properties for _Z28flash_fwd_mla_combine_kernelIN7cutlass6half_tELi512ELi8ELi64ELi256EEv14DecodingParams
    24 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, used 1 barriers, 24 bytes cumulative stack size
ptxas info    : Compile time = 0.000 ms
ptxas info    : Compiling entry function '_Z28flash_fwd_mla_combine_kernelIN7cutlass6half_tELi512ELi8ELi32ELi256EEv14DecodingParams' for 'sm_120'
ptxas info    : Function properties for _Z28flash_fwd_mla_combine_kernelIN7cutlass6half_tELi512ELi8ELi32ELi256EEv14DecodingParams
    24 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, used 1 barriers, 24 bytes cumulative stack size
ptxas info    : Compile time = 0.000 ms
ptxas info    : Compiling entry function '_Z28flash_fwd_mla_combine_kernelIN7cutlass10bfloat16_tELi512ELi8ELi64ELi256EEv14DecodingParams' for 'sm_120'
ptxas info    : Function properties for _Z28flash_fwd_mla_combine_kernelIN7cutlass10bfloat16_tELi512ELi8ELi64ELi256EEv14DecodingParams
    24 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, used 1 barriers, 24 bytes cumulative stack size
ptxas info    : Compile time = 0.000 ms
ptxas info    : Compiling entry function '_Z28flash_fwd_mla_combine_kernelIN7cutlass10bfloat16_tELi512ELi8ELi32ELi256EEv14DecodingParams' for 'sm_120'
ptxas info    : Function properties for _Z28flash_fwd_mla_combine_kernelIN7cutlass10bfloat16_tELi512ELi8ELi32ELi256EEv14DecodingParams
    24 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, used 1 barriers, 24 bytes cumulative stack size
ptxas info    : Compile time = 0.000 ms
tmpxft_0000695c_00000000-7_mla_combine.cudafe1.cpp
[3/4] cl /showIncludes /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm90 -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\tools\util\include "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\include\torch\csrc\api\include" "-IC:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Include" "-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.44.35207\include" "-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\VS\include" "-IC:\Program Files (x86)\Windows Kits\10\include\10.0.26100.0\ucrt" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\um" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\shared" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\winrt" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\cppwinrt" "-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um" /MD /wd4819 /wd4251 /wd4244 /wd4267 /wd4275 /wd4018 /wd4190 /wd4624 /wd4067 /wd4068 /EHsc -c C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\pybind.cpp /FoC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\pybind.obj /O2 /std:c++17 /Zc:__cplusplus /EHsc /permissive- /DNOMINMAX /DWIN32_LEAN_AND_MEAN /D_HAS_EXCEPTIONS=1 /utf-8 /DNDEBUG /W0 /FImsvc_compat.h /DFLASH_MLA_DISABLE_SM90 /DFLASH_MLA_DISABLE_SM100 /DFLASH_MLA_SM120_DISABLE_BWD -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=cuda
cl : Command line warning D9025 : overriding '/W3' with '/W0'
[4/4] C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\bin\nvcc --generate-dependencies-with-compile --dependency-output C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.obj.d -std=c++17 -Xcompiler /MD -Xcompiler /wd4819 -Xcompiler /wd4251 -Xcompiler /wd4244 -Xcompiler /wd4267 -Xcompiler /wd4275 -Xcompiler /wd4018 -Xcompiler /wd4190 -Xcompiler /wd4624 -Xcompiler /wd4067 -Xcompiler /wd4068 -Xcompiler /EHsc --use-local-env -Xcudafe --diag_suppress=base_class_has_different_dll_interface -Xcudafe --diag_suppress=field_without_dll_interface -Xcudafe --diag_suppress=dll_interface_conflict_none_assumed -Xcudafe --diag_suppress=dll_interface_conflict_dllexport_assumed -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm90 -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include -IC:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\tools\util\include "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\include\torch\csrc\api\include" "-IC:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\include" "-IC:\Users\Shashank Murthy\.conda\envs\150BLLM\Include" "-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.44.35207\include" "-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\VS\include" "-IC:\Program Files (x86)\Windows Kits\10\include\10.0.26100.0\ucrt" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\um" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\shared" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\winrt" "-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.26100.0\\cppwinrt" "-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um" -c C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu -o C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.obj -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -include msvc_compat.h -O3 -std=c++17 -DNDEBUG -D_USE_MATH_DEFINES -Wno-deprecated-declarations -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --ptxas-options=-v,--register-usage-level=10 -Xcompiler /Zc:__cplusplus -Xcompiler /permissive- -Xcompiler /Zc:__cplusplus -DFLASH_MLA_DISABLE_SM90 -DFLASH_MLA_DISABLE_SM100 -DFLASH_MLA_SM120_DISABLE_BWD -gencode arch=compute_120,code=sm_120 -gencode arch=compute_120,code=compute_120 --threads 32 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=cuda
fmha_cutlass_fwd_sm100.cu
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_OPERATORS__' with '/U__CUDA_NO_HALF_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_CONVERSIONS__' with '/U__CUDA_NO_HALF_CONVERSIONS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF2_OPERATORS__' with '/U__CUDA_NO_HALF2_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_BFLOAT16_CONVERSIONS__' with '/U__CUDA_NO_BFLOAT16_CONVERSIONS__'
fmha_cutlass_fwd_sm100.cu
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_OPERATORS__' with '/U__CUDA_NO_HALF_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_CONVERSIONS__' with '/U__CUDA_NO_HALF_CONVERSIONS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF2_OPERATORS__' with '/U__CUDA_NO_HALF2_OPERATORS__'
cl : Command line warning D9025 : overriding '/D__CUDA_NO_BFLOAT16_CONVERSIONS__' with '/U__CUDA_NO_BFLOAT16_CONVERSIONS__'
fmha_cutlass_fwd_sm100.cu
C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/fmha_fusion.hpp(129): warning #221-D: floating-point value does not fit in required floating-point type
          acc_qk(i) = -((float)(1e+300));
                        ^

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/fmha_fusion.hpp(181): warning #221-D: floating-point value does not fit in required floating-point type
          acc_qk(i) = -((float)(1e+300));
                        ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/fmha_fusion.hpp(263): warning #221-D: floating-point value does not fit in required floating-point type
            acc_qk(i) = -((float)(1e+300));
                          ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/fmha_fusion.hpp(272): warning #221-D: floating-point value does not fit in required floating-point type
            acc_qk(i) = -((float)(1e+300));
                          ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/fmha_fusion.hpp(309): warning #221-D: floating-point value does not fit in required floating-point type
          acc_qk(i) = -((float)(1e+300));
                        ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/sm100_fmha_fwd_mainloop_tma_warpspecialized.hpp(594): warning #221-D: floating-point value does not fit in required floating-point type
      ElementQK row_max_safe = row_max == -((float)(1e+300)) ? 0 : row_max;
                                            ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/sm100_fmha_fwd_mainloop_tma_warpspecialized.hpp(725): warning #221-D: floating-point value does not fit in required floating-point type
      ElementQK row_max = -((float)(1e+300));
                            ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/sm100_fmha_fwd_mainloop_tma_warpspecialized.hpp(1155): warning #221-D: floating-point value does not fit in required floating-point type
      float lse = -((float)(1e+300));
                    ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/sm100_fmha_mla_fwd_mainloop_tma_warpspecialized.hpp(618): warning #221-D: floating-point value does not fit in required floating-point type
      ElementQK row_max_safe = row_max == -((float)(1e+300)) ? 0 : row_max;
                                            ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/sm100_fmha_mla_fwd_mainloop_tma_warpspecialized.hpp(750): warning #221-D: floating-point value does not fit in required floating-point type
      ElementQK row_max = -((float)(1e+300));
                            ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\collective/sm100_fmha_mla_fwd_mainloop_tma_warpspecialized.hpp(1165): warning #221-D: floating-point value does not fit in required floating-point type
      float lse = -((float)(1e+300));
                    ^

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(184): warning #550-D: variable "problem_size" was set but never used
      decltype(problem_shape_in) problem_size;
                                 ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 335
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 93 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(196): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 335
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 93 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(184): warning #550-D: variable "problem_size" was set but never used
      decltype(problem_shape_in) problem_size;
                                 ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 339
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 93 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(196): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 339
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 93 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(196): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 335
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=false, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::false_type]" at line 97 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(196): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 339
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=false, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::false_type]" at line 97 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(184): warning #550-D: variable "problem_size" was set but never used
      decltype(problem_shape_in) problem_size;
                                 ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 335
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 93 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(196): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 335
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 93 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(184): warning #550-D: variable "problem_size" was set but never used
      decltype(problem_shape_in) problem_size;
                                 ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 339
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 93 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(196): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 339
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 93 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(196): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 335
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=false, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::false_type]" at line 97 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(196): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 339
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=false, ActiveMask=cutlass::fmha::collective::CausalMask<false>, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::CausalMask<false>, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::false_type]" at line 97 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(184): warning #550-D: variable "problem_size" was set but never used
      decltype(problem_shape_in) problem_size;
                                 ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 335
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 93 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(196): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 335
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 93 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(184): warning #550-D: variable "problem_size" was set but never used
      decltype(problem_shape_in) problem_size;
                                 ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 339
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 93 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(196): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=MlaOptions]" at line 339
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=true, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 93 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(196): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=true, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 335
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=false, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::false_type]" at line 97 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(196): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=false, kIsVarlen=true, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>, Options=FmhaOptions]" at line 339
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=true, kIsMla=false, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::false_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::true_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::false_type]" at line 97 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(184): warning #550-D: variable "problem_size" was set but never used
      decltype(problem_shape_in) problem_size;
                                 ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::true_type>>, Options=MlaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::true_type>>, Options=MlaOptions]" at line 335
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::true_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 93 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(196): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::true_type>>, Options=MlaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::true_type>>, Options=MlaOptions]" at line 335
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::true_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 93 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(184): warning #550-D: variable "problem_size" was set but never used
      decltype(problem_shape_in) problem_size;
                                 ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::true_type>>, Options=MlaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::true_type>>, Options=MlaOptions]" at line 339
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::true_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 93 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(196): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::true_type>>, Options=MlaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=true, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::true_type>>, Options=MlaOptions]" at line 339
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=true, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::true_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::true_type]" at line 93 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(196): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::true_type>>, Options=FmhaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=true, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::true_type>>, Options=FmhaOptions]" at line 335
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=false, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::true_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::false_type]" at line 97 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cuh(196): warning #177-D: variable "get_head_dimension" was declared but never referenced
      auto get_head_dimension = [&]() {
           ^
          detected during:
            instantiation of "FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::ProblemShapeType FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::initialize(const Options &, int, int, int, int, void *, void *) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::true_type>>, Options=FmhaOptions]" at line 242
            instantiation of "void FwdRunner<KernelTraits, kIsMla, kIsMaskTileSchedulerValid, kIsVarlen, Element_, ElementOut_, ActiveMask, KernelOptions...>::run(const Options &, const cutlass::KernelHardwareInfo &, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, at::Tensor, at::Tensor, at::Tensor, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, kIsMla=false, kIsMaskTileSchedulerValid=false, kIsVarlen=false, Element_=cutlass::bfloat16_t, ElementOut_=cutlass::bfloat16_t, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::true_type>>, Options=FmhaOptions]" at line 339
            instantiation of "void run_fmha_fwd<KernelTraits,DTypeIn,DTypeOut,kIsVarlen,kIsMla,ActiveMask,KernelOptions...>(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with KernelTraits=flash::Sm120WorkstationConfig, DTypeIn=cutlass::bfloat16_t, DTypeOut=cutlass::bfloat16_t, kIsVarlen=false, kIsMla=false, ActiveMask=cutlass::fmha::collective::ResidualMask, KernelOptions=<cutlass::fmha::kernel::Option<cutlass::fmha::kernel::Tag::kIsPersistent, cute::true_type>>]" at line 42 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu
            instantiation of "void call_run_fmha_fwd(Mask, Varlen, Element, ElementOut, Mla, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int, int) [with Mask=cutlass::fmha::collective::ResidualMask, Varlen=cute::false_type, Element=cutlass::bfloat16_t, ElementOut=cutlass::bfloat16_t, Mla=cute::false_type]" at line 97 of C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.cu

ptxas info    : 63 bytes gmem
ptxas info    : Compiling entry function '_ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJiiiNS7_IJNS7_IJiiEEEiEEEEEENS1_10collective38Sm100FmhaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSE_ILi32EEESF_EEENS7_IJiNSE_ILi1EEES9_EEENS7_IJiSI_NS7_IJNS7_IJNSE_ILi0EEEiEEEiEEEEEESN_NSB_12ResidualMaskENS7_IJSI_SI_SI_EEENSE_ILb0EEEEENSB_38Sm100FmhaFwdEpilogueTmaWarpspecializedISD_fNS7_IJSF_SF_SG_EEESJ_NS7_IJSI_S9_EEESQ_EENS2_23IndividualTileSchedulerENS2_41Sm100FmhaCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE' for 'sm_120'
ptxas info    : Function properties for _ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJiiiNS7_IJNS7_IJiiEEEiEEEEEENS1_10collective38Sm100FmhaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSE_ILi32EEESF_EEENS7_IJiNSE_ILi1EEES9_EEENS7_IJiSI_NS7_IJNS7_IJNSE_ILi0EEEiEEEiEEEEEESN_NSB_12ResidualMaskENS7_IJSI_SI_SI_EEENSE_ILb0EEEEENSB_38Sm100FmhaFwdEpilogueTmaWarpspecializedISD_fNS7_IJSF_SF_SG_EEESJ_NS7_IJSI_S9_EEESQ_EENS2_23IndividualTileSchedulerENS2_41Sm100FmhaCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 24 registers, used 0 barriers
ptxas info    : Compile time = 0.000 ms
ptxas info    : Compiling entry function '_ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJiiiNS7_IJNS7_IJiiEEEiEEEEEENS1_10collective38Sm100FmhaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSE_ILi32EEESF_EEENS7_IJiNSE_ILi1EEES9_EEENS7_IJiSI_NS7_IJNS7_IJNSE_ILi0EEEiEEEiEEEEEESN_NSB_12ResidualMaskENS7_IJSI_SI_SI_EEENSE_ILb0EEEEENSB_38Sm100FmhaFwdEpilogueTmaWarpspecializedISD_fNS7_IJSF_SF_SG_EEESJ_NS7_IJSI_S9_EEESQ_EENS2_29CausalIndividualTileSchedulerENS2_41Sm100FmhaCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE' for 'sm_120'
ptxas info    : Function properties for _ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJiiiNS7_IJNS7_IJiiEEEiEEEEEENS1_10collective38Sm100FmhaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSE_ILi32EEESF_EEENS7_IJiNSE_ILi1EEES9_EEENS7_IJiSI_NS7_IJNS7_IJNSE_ILi0EEEiEEEiEEEEEESN_NSB_12ResidualMaskENS7_IJSI_SI_SI_EEENSE_ILb0EEEEENSB_38Sm100FmhaFwdEpilogueTmaWarpspecializedISD_fNS7_IJSF_SF_SG_EEESJ_NS7_IJSI_S9_EEESQ_EENS2_29CausalIndividualTileSchedulerENS2_41Sm100FmhaCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 24 registers, used 0 barriers
ptxas info    : Compile time = 0.000 ms
ptxas info    : Compiling entry function '_ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJiiNS7_IJiiEEENS7_IJS8_iEEEEEENS1_10collective37Sm100MlaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSE_ILi32EEENS7_IJSF_SF_EEEEEENS7_IJiNSE_ILi1EEES9_EEENS7_IJiSJ_NS7_IJNS7_IJNSE_ILi0EEEiEEEiEEEEEESO_NSB_12ResidualMaskENS7_IJSJ_SJ_SJ_EEENSE_ILb0EEEEENSB_38Sm100FmhaFwdEpilogueTmaWarpspecializedISD_fNS7_IJSF_SF_SG_EEESK_NS7_IJSJ_S9_EEESR_EENS2_23IndividualTileSchedulerENS2_43Sm100MlaFwdCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE' for 'sm_120'
ptxas info    : Function properties for _ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJiiNS7_IJiiEEENS7_IJS8_iEEEEEENS1_10collective37Sm100MlaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSE_ILi32EEENS7_IJSF_SF_EEEEEENS7_IJiNSE_ILi1EEES9_EEENS7_IJiSJ_NS7_IJNS7_IJNSE_ILi0EEEiEEEiEEEEEESO_NSB_12ResidualMaskENS7_IJSJ_SJ_SJ_EEENSE_ILb0EEEEENSB_38Sm100FmhaFwdEpilogueTmaWarpspecializedISD_fNS7_IJSF_SF_SG_EEESK_NS7_IJSJ_S9_EEESR_EENS2_23IndividualTileSchedulerENS2_43Sm100MlaFwdCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 24 registers, used 0 barriers
ptxas info    : Compile time = 0.000 ms
ptxas info    : Compiling entry function '_ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJiiNS7_IJiiEEENS7_IJS8_iEEEEEENS1_10collective37Sm100MlaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSE_ILi32EEENS7_IJSF_SF_EEEEEENS7_IJiNSE_ILi1EEES9_EEENS7_IJiSJ_NS7_IJNS7_IJNSE_ILi0EEEiEEEiEEEEEESO_NSB_12ResidualMaskENS7_IJSJ_SJ_SJ_EEENSE_ILb0EEEEENSB_38Sm100FmhaFwdEpilogueTmaWarpspecializedISD_fNS7_IJSF_SF_SG_EEESK_NS7_IJSJ_S9_EEESR_EENS2_29CausalIndividualTileSchedulerENS2_43Sm100MlaFwdCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE' for 'sm_120'
ptxas info    : Function properties for _ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJiiNS7_IJiiEEENS7_IJS8_iEEEEEENS1_10collective37Sm100MlaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSE_ILi32EEENS7_IJSF_SF_EEEEEENS7_IJiNSE_ILi1EEES9_EEENS7_IJiSJ_NS7_IJNS7_IJNSE_ILi0EEEiEEEiEEEEEESO_NSB_12ResidualMaskENS7_IJSJ_SJ_SJ_EEENSE_ILb0EEEEENSB_38Sm100FmhaFwdEpilogueTmaWarpspecializedISD_fNS7_IJSF_SF_SG_EEESK_NS7_IJSJ_S9_EEESR_EENS2_29CausalIndividualTileSchedulerENS2_43Sm100MlaFwdCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 24 registers, used 0 barriers
ptxas info    : Compile time = 0.000 ms
ptxas info    : Compiling entry function '_ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJNS1_10collective14VariableLengthES9_iNS7_IJNS7_IJiiEEEiEEEEEENS8_38Sm100FmhaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSF_ILi32EEESG_EEENS7_IJiNSF_ILi1EEESB_EEENS7_IJiSJ_NS7_IJNS7_IJNSF_ILi0EEEiEEEiEEEEEESO_NS8_12ResidualMaskENS7_IJSJ_SJ_SJ_EEENSF_ILb0EEEEENS8_38Sm100FmhaFwdEpilogueTmaWarpspecializedISE_fNS7_IJSG_SG_SH_EEESK_NS7_IJSJ_SB_EEESR_EENS2_23IndividualTileSchedulerENS2_41Sm100FmhaCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE' for 'sm_120'
ptxas info    : Function properties for _ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJNS1_10collective14VariableLengthES9_iNS7_IJNS7_IJiiEEEiEEEEEENS8_38Sm100FmhaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSF_ILi32EEESG_EEENS7_IJiNSF_ILi1EEESB_EEENS7_IJiSJ_NS7_IJNS7_IJNSF_ILi0EEEiEEEiEEEEEESO_NS8_12ResidualMaskENS7_IJSJ_SJ_SJ_EEENSF_ILb0EEEEENS8_38Sm100FmhaFwdEpilogueTmaWarpspecializedISE_fNS7_IJSG_SG_SH_EEESK_NS7_IJSJ_SB_EEESR_EENS2_23IndividualTileSchedulerENS2_41Sm100FmhaCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 24 registers, used 0 barriers
ptxas info    : Compile time = 0.000 ms
ptxas info    : Compiling entry function '_ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJNS1_10collective14VariableLengthES9_iNS7_IJNS7_IJiiEEEiEEEEEENS8_38Sm100FmhaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSF_ILi32EEESG_EEENS7_IJiNSF_ILi1EEESB_EEENS7_IJiSJ_NS7_IJNS7_IJNSF_ILi0EEEiEEEiEEEEEESO_NS8_12ResidualMaskENS7_IJSJ_SJ_SJ_EEENSF_ILb0EEEEENS8_38Sm100FmhaFwdEpilogueTmaWarpspecializedISE_fNS7_IJSG_SG_SH_EEESK_NS7_IJSJ_SB_EEESR_EENS2_29CausalIndividualTileSchedulerENS2_41Sm100FmhaCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE' for 'sm_120'
ptxas info    : Function properties for _ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJNS1_10collective14VariableLengthES9_iNS7_IJNS7_IJiiEEEiEEEEEENS8_38Sm100FmhaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSF_ILi32EEESG_EEENS7_IJiNSF_ILi1EEESB_EEENS7_IJiSJ_NS7_IJNS7_IJNSF_ILi0EEEiEEEiEEEEEESO_NS8_12ResidualMaskENS7_IJSJ_SJ_SJ_EEENSF_ILb0EEEEENS8_38Sm100FmhaFwdEpilogueTmaWarpspecializedISE_fNS7_IJSG_SG_SH_EEESK_NS7_IJSJ_SB_EEESR_EENS2_29CausalIndividualTileSchedulerENS2_41Sm100FmhaCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 24 registers, used 0 barriers
ptxas info    : Compile time = 0.000 ms
ptxas info    : Compiling entry function '_ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJNS1_10collective14VariableLengthES9_NS7_IJiiEEENS7_IJSA_iEEEEEENS8_37Sm100MlaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSF_ILi32EEENS7_IJSG_SG_EEEEEENS7_IJiNSF_ILi1EEESB_EEENS7_IJiSK_NS7_IJNS7_IJNSF_ILi0EEEiEEEiEEEEEESP_NS8_12ResidualMaskENS7_IJSK_SK_SK_EEENSF_ILb0EEEEENS8_38Sm100FmhaFwdEpilogueTmaWarpspecializedISE_fNS7_IJSG_SG_SH_EEESL_NS7_IJSK_SB_EEESS_EENS2_23IndividualTileSchedulerENS2_43Sm100MlaFwdCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE' for 'sm_120'
ptxas info    : Function properties for _ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJNS1_10collective14VariableLengthES9_NS7_IJiiEEENS7_IJSA_iEEEEEENS8_37Sm100MlaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSF_ILi32EEENS7_IJSG_SG_EEEEEENS7_IJiNSF_ILi1EEESB_EEENS7_IJiSK_NS7_IJNS7_IJNSF_ILi0EEEiEEEiEEEEEESP_NS8_12ResidualMaskENS7_IJSK_SK_SK_EEENSF_ILb0EEEEENS8_38Sm100FmhaFwdEpilogueTmaWarpspecializedISE_fNS7_IJSG_SG_SH_EEESL_NS7_IJSK_SB_EEESS_EENS2_23IndividualTileSchedulerENS2_43Sm100MlaFwdCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 24 registers, used 0 barriers
ptxas info    : Compile time = 0.000 ms
ptxas info    : Compiling entry function '_ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJNS1_10collective14VariableLengthES9_NS7_IJiiEEENS7_IJSA_iEEEEEENS8_37Sm100MlaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSF_ILi32EEENS7_IJSG_SG_EEEEEENS7_IJiNSF_ILi1EEESB_EEENS7_IJiSK_NS7_IJNS7_IJNSF_ILi0EEEiEEEiEEEEEESP_NS8_12ResidualMaskENS7_IJSK_SK_SK_EEENSF_ILb0EEEEENS8_38Sm100FmhaFwdEpilogueTmaWarpspecializedISE_fNS7_IJSG_SG_SH_EEESL_NS7_IJSK_SB_EEESS_EENS2_29CausalIndividualTileSchedulerENS2_43Sm100MlaFwdCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE' for 'sm_120'
ptxas info    : Function properties for _ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJNS1_10collective14VariableLengthES9_NS7_IJiiEEENS7_IJSA_iEEEEEENS8_37Sm100MlaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSF_ILi32EEENS7_IJSG_SG_EEEEEENS7_IJiNSF_ILi1EEESB_EEENS7_IJiSK_NS7_IJNS7_IJNSF_ILi0EEEiEEEiEEEEEESP_NS8_12ResidualMaskENS7_IJSK_SK_SK_EEENSF_ILb0EEEEENS8_38Sm100FmhaFwdEpilogueTmaWarpspecializedISE_fNS7_IJSG_SG_SH_EEESL_NS7_IJSK_SB_EEESS_EENS2_29CausalIndividualTileSchedulerENS2_43Sm100MlaFwdCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 24 registers, used 0 barriers
ptxas info    : Compile time = 0.000 ms
ptxas info    : Compiling entry function '_ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJiiiNS7_IJNS7_IJiiEEEiEEEEEENS1_10collective38Sm100FmhaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSE_ILi32EEESF_EEENS7_IJiNSE_ILi1EEES9_EEENS7_IJiSI_NS7_IJNS7_IJNSE_ILi0EEEiEEEiEEEEEESN_NSB_10CausalMaskILb0EEENS7_IJSI_SI_SI_EEENSE_ILb0EEEEENSB_38Sm100FmhaFwdEpilogueTmaWarpspecializedISD_fNS7_IJSF_SF_SG_EEESJ_NS7_IJSI_S9_EEESR_EENS2_23IndividualTileSchedulerENS2_41Sm100FmhaCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE' for 'sm_120'
ptxas info    : Function properties for _ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJiiiNS7_IJNS7_IJiiEEEiEEEEEENS1_10collective38Sm100FmhaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSE_ILi32EEESF_EEENS7_IJiNSE_ILi1EEES9_EEENS7_IJiSI_NS7_IJNS7_IJNSE_ILi0EEEiEEEiEEEEEESN_NSB_10CausalMaskILb0EEENS7_IJSI_SI_SI_EEENSE_ILb0EEEEENSB_38Sm100FmhaFwdEpilogueTmaWarpspecializedISD_fNS7_IJSF_SF_SG_EEESJ_NS7_IJSI_S9_EEESR_EENS2_23IndividualTileSchedulerENS2_41Sm100FmhaCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 24 registers, used 0 barriers
ptxas info    : Compile time = 0.000 ms
ptxas info    : Compiling entry function '_ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJiiiNS7_IJNS7_IJiiEEEiEEEEEENS1_10collective38Sm100FmhaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSE_ILi32EEESF_EEENS7_IJiNSE_ILi1EEES9_EEENS7_IJiSI_NS7_IJNS7_IJNSE_ILi0EEEiEEEiEEEEEESN_NSB_10CausalMaskILb0EEENS7_IJSI_SI_SI_EEENSE_ILb0EEEEENSB_38Sm100FmhaFwdEpilogueTmaWarpspecializedISD_fNS7_IJSF_SF_SG_EEESJ_NS7_IJSI_S9_EEESR_EENS2_29CausalIndividualTileSchedulerENS2_41Sm100FmhaCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE' for 'sm_120'
ptxas info    : Function properties for _ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJiiiNS7_IJNS7_IJiiEEEiEEEEEENS1_10collective38Sm100FmhaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSE_ILi32EEESF_EEENS7_IJiNSE_ILi1EEES9_EEENS7_IJiSI_NS7_IJNS7_IJNSE_ILi0EEEiEEEiEEEEEESN_NSB_10CausalMaskILb0EEENS7_IJSI_SI_SI_EEENSE_ILb0EEEEENSB_38Sm100FmhaFwdEpilogueTmaWarpspecializedISD_fNS7_IJSF_SF_SG_EEESJ_NS7_IJSI_S9_EEESR_EENS2_29CausalIndividualTileSchedulerENS2_41Sm100FmhaCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 24 registers, used 0 barriers
ptxas info    : Compile time = 0.000 ms
ptxas info    : Compiling entry function '_ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJiiNS7_IJiiEEENS7_IJS8_iEEEEEENS1_10collective37Sm100MlaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSE_ILi32EEENS7_IJSF_SF_EEEEEENS7_IJiNSE_ILi1EEES9_EEENS7_IJiSJ_NS7_IJNS7_IJNSE_ILi0EEEiEEEiEEEEEESO_NSB_10CausalMaskILb0EEENS7_IJSJ_SJ_SJ_EEENSE_ILb0EEEEENSB_38Sm100FmhaFwdEpilogueTmaWarpspecializedISD_fNS7_IJSF_SF_SG_EEESK_NS7_IJSJ_S9_EEESS_EENS2_23IndividualTileSchedulerENS2_43Sm100MlaFwdCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE' for 'sm_120'
ptxas info    : Function properties for _ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJiiNS7_IJiiEEENS7_IJS8_iEEEEEENS1_10collective37Sm100MlaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSE_ILi32EEENS7_IJSF_SF_EEEEEENS7_IJiNSE_ILi1EEES9_EEENS7_IJiSJ_NS7_IJNS7_IJNSE_ILi0EEEiEEEiEEEEEESO_NSB_10CausalMaskILb0EEENS7_IJSJ_SJ_SJ_EEENSE_ILb0EEEEENSB_38Sm100FmhaFwdEpilogueTmaWarpspecializedISD_fNS7_IJSF_SF_SG_EEESK_NS7_IJSJ_S9_EEESS_EENS2_23IndividualTileSchedulerENS2_43Sm100MlaFwdCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 24 registers, used 0 barriers
ptxas info    : Compile time = 0.000 ms
ptxas info    : Compiling entry function '_ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJiiNS7_IJiiEEENS7_IJS8_iEEEEEENS1_10collective37Sm100MlaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSE_ILi32EEENS7_IJSF_SF_EEEEEENS7_IJiNSE_ILi1EEES9_EEENS7_IJiSJ_NS7_IJNS7_IJNSE_ILi0EEEiEEEiEEEEEESO_NSB_10CausalMaskILb0EEENS7_IJSJ_SJ_SJ_EEENSE_ILb0EEEEENSB_38Sm100FmhaFwdEpilogueTmaWarpspecializedISD_fNS7_IJSF_SF_SG_EEESK_NS7_IJSJ_S9_EEESS_EENS2_29CausalIndividualTileSchedulerENS2_43Sm100MlaFwdCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE' for 'sm_120'
ptxas info    : Function properties for _ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJiiNS7_IJiiEEENS7_IJS8_iEEEEEENS1_10collective37Sm100MlaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSE_ILi32EEENS7_IJSF_SF_EEEEEENS7_IJiNSE_ILi1EEES9_EEENS7_IJiSJ_NS7_IJNS7_IJNSE_ILi0EEEiEEEiEEEEEESO_NSB_10CausalMaskILb0EEENS7_IJSJ_SJ_SJ_EEENSE_ILb0EEEEENSB_38Sm100FmhaFwdEpilogueTmaWarpspecializedISD_fNS7_IJSF_SF_SG_EEESK_NS7_IJSJ_S9_EEESS_EENS2_29CausalIndividualTileSchedulerENS2_43Sm100MlaFwdCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 24 registers, used 0 barriers
ptxas info    : Compile time = 0.000 ms
ptxas info    : Compiling entry function '_ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJNS1_10collective14VariableLengthES9_iNS7_IJNS7_IJiiEEEiEEEEEENS8_38Sm100FmhaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSF_ILi32EEESG_EEENS7_IJiNSF_ILi1EEESB_EEENS7_IJiSJ_NS7_IJNS7_IJNSF_ILi0EEEiEEEiEEEEEESO_NS8_10CausalMaskILb0EEENS7_IJSJ_SJ_SJ_EEENSF_ILb0EEEEENS8_38Sm100FmhaFwdEpilogueTmaWarpspecializedISE_fNS7_IJSG_SG_SH_EEESK_NS7_IJSJ_SB_EEESS_EENS2_23IndividualTileSchedulerENS2_41Sm100FmhaCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE' for 'sm_120'
ptxas info    : Function properties for _ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJNS1_10collective14VariableLengthES9_iNS7_IJNS7_IJiiEEEiEEEEEENS8_38Sm100FmhaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSF_ILi32EEESG_EEENS7_IJiNSF_ILi1EEESB_EEENS7_IJiSJ_NS7_IJNS7_IJNSF_ILi0EEEiEEEiEEEEEESO_NS8_10CausalMaskILb0EEENS7_IJSJ_SJ_SJ_EEENSF_ILb0EEEEENS8_38Sm100FmhaFwdEpilogueTmaWarpspecializedISE_fNS7_IJSG_SG_SH_EEESK_NS7_IJSJ_SB_EEESS_EENS2_23IndividualTileSchedulerENS2_41Sm100FmhaCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 24 registers, used 0 barriers
ptxas info    : Compile time = 0.000 ms
ptxas info    : Compiling entry function '_ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJNS1_10collective14VariableLengthES9_iNS7_IJNS7_IJiiEEEiEEEEEENS8_38Sm100FmhaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSF_ILi32EEESG_EEENS7_IJiNSF_ILi1EEESB_EEENS7_IJiSJ_NS7_IJNS7_IJNSF_ILi0EEEiEEEiEEEEEESO_NS8_10CausalMaskILb0EEENS7_IJSJ_SJ_SJ_EEENSF_ILb0EEEEENS8_38Sm100FmhaFwdEpilogueTmaWarpspecializedISE_fNS7_IJSG_SG_SH_EEESK_NS7_IJSJ_SB_EEESS_EENS2_29CausalIndividualTileSchedulerENS2_41Sm100FmhaCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE' for 'sm_120'
ptxas info    : Function properties for _ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJNS1_10collective14VariableLengthES9_iNS7_IJNS7_IJiiEEEiEEEEEENS8_38Sm100FmhaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSF_ILi32EEESG_EEENS7_IJiNSF_ILi1EEESB_EEENS7_IJiSJ_NS7_IJNS7_IJNSF_ILi0EEEiEEEiEEEEEESO_NS8_10CausalMaskILb0EEENS7_IJSJ_SJ_SJ_EEENSF_ILb0EEEEENS8_38Sm100FmhaFwdEpilogueTmaWarpspecializedISE_fNS7_IJSG_SG_SH_EEESK_NS7_IJSJ_SB_EEESS_EENS2_29CausalIndividualTileSchedulerENS2_41Sm100FmhaCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 24 registers, used 0 barriers
ptxas info    : Compile time = 0.000 ms
ptxas info    : Compiling entry function '_ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJNS1_10collective14VariableLengthES9_NS7_IJiiEEENS7_IJSA_iEEEEEENS8_37Sm100MlaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSF_ILi32EEENS7_IJSG_SG_EEEEEENS7_IJiNSF_ILi1EEESB_EEENS7_IJiSK_NS7_IJNS7_IJNSF_ILi0EEEiEEEiEEEEEESP_NS8_10CausalMaskILb0EEENS7_IJSK_SK_SK_EEENSF_ILb0EEEEENS8_38Sm100FmhaFwdEpilogueTmaWarpspecializedISE_fNS7_IJSG_SG_SH_EEESL_NS7_IJSK_SB_EEEST_EENS2_23IndividualTileSchedulerENS2_43Sm100MlaFwdCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE' for 'sm_120'
ptxas info    : Function properties for _ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJNS1_10collective14VariableLengthES9_NS7_IJiiEEENS7_IJSA_iEEEEEENS8_37Sm100MlaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSF_ILi32EEENS7_IJSG_SG_EEEEEENS7_IJiNSF_ILi1EEESB_EEENS7_IJiSK_NS7_IJNS7_IJNSF_ILi0EEEiEEEiEEEEEESP_NS8_10CausalMaskILb0EEENS7_IJSK_SK_SK_EEENSF_ILb0EEEEENS8_38Sm100FmhaFwdEpilogueTmaWarpspecializedISE_fNS7_IJSG_SG_SH_EEESL_NS7_IJSK_SB_EEEST_EENS2_23IndividualTileSchedulerENS2_43Sm100MlaFwdCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 24 registers, used 0 barriers
ptxas info    : Compile time = 0.000 ms
ptxas info    : Compiling entry function '_ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJNS1_10collective14VariableLengthES9_NS7_IJiiEEENS7_IJSA_iEEEEEENS8_37Sm100MlaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSF_ILi32EEENS7_IJSG_SG_EEEEEENS7_IJiNSF_ILi1EEESB_EEENS7_IJiSK_NS7_IJNS7_IJNSF_ILi0EEEiEEEiEEEEEESP_NS8_10CausalMaskILb0EEENS7_IJSK_SK_SK_EEENSF_ILb0EEEEENS8_38Sm100FmhaFwdEpilogueTmaWarpspecializedISE_fNS7_IJSG_SG_SH_EEESL_NS7_IJSK_SB_EEEST_EENS2_29CausalIndividualTileSchedulerENS2_43Sm100MlaFwdCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE' for 'sm_120'
ptxas info    : Function properties for _ZN7cutlass13device_kernelINS_4fmha6kernel36Sm100FmhaFwdKernelTmaWarpspecializedIN5flash22Sm120WorkstationConfigEN4cute5tupleIJNS1_10collective14VariableLengthES9_NS7_IJiiEEENS7_IJSA_iEEEEEENS8_37Sm100MlaFwdMainloopTmaWarpspecializedINS_10bfloat16_tEffNS7_IJNS6_1CILi64EEENSF_ILi32EEENS7_IJSG_SG_EEEEEENS7_IJiNSF_ILi1EEESB_EEENS7_IJiSK_NS7_IJNS7_IJNSF_ILi0EEEiEEEiEEEEEESP_NS8_10CausalMaskILb0EEENS7_IJSK_SK_SK_EEENSF_ILb0EEEEENS8_38Sm100FmhaFwdEpilogueTmaWarpspecializedISE_fNS7_IJSG_SG_SH_EEESL_NS7_IJSK_SB_EEEST_EENS2_29CausalIndividualTileSchedulerENS2_43Sm100MlaFwdCtxKernelWarpspecializedScheduleEEEEEvNT_6ParamsE
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 24 registers, used 0 barriers
ptxas info    : Compile time = 0.000 ms
tmpxft_00002340_00000000-7_fmha_cutlass_fwd_sm100.cudafe1.cpp
C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include\cute\numeric\arithmetic_tuple.hpp(277) : warning C4172: returning address of local variable or temporary 
C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include\cute\numeric\arithmetic_tuple.hpp(277) : warning C4172: returning address of local variable or temporary 
C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include\cute\numeric\arithmetic_tuple.hpp(277) : warning C4172: returning address of local variable or temporary 
C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include\cute\numeric\arithmetic_tuple.hpp(277) : warning C4172: returning address of local variable or temporary 
C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include\cute\numeric\arithmetic_tuple.hpp(277) : warning C4172: returning address of local variable or temporary 
C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include\cute\numeric\arithmetic_tuple.hpp(277) : warning C4172: returning address of local variable or temporary 
C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include\cute\numeric\arithmetic_tuple.hpp(277) : warning C4172: returning address of local variable or temporary 
C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include\cute\numeric\arithmetic_tuple.hpp(277) : warning C4172: returning address of local variable or temporary 
C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include\cute\numeric\arithmetic_tuple.hpp(277) : warning C4172: returning address of local variable or temporary 
C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include\cute\numeric\arithmetic_tuple.hpp(277) : warning C4172: returning address of local variable or temporary 
C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include\cute\numeric\arithmetic_tuple.hpp(277) : warning C4172: returning address of local variable or temporary 
C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include\cute\numeric\arithmetic_tuple.hpp(277) : warning C4172: returning address of local variable or temporary 
C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include\cute\numeric\arithmetic_tuple.hpp(277) : warning C4172: returning address of local variable or temporary 
C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include\cute\numeric\arithmetic_tuple.hpp(277) : warning C4172: returning address of local variable or temporary 
C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\csrc\cutlass\include\cute\numeric\arithmetic_tuple.hpp(277) : warning C4172: returning address of local variable or temporary 
creating C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\lib.win-amd64-cpython-312\flash_mla
"C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.44.35207\bin\HostX86\x64\link.exe" /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO "/LIBPATH:C:\Users\Shashank Murthy\.conda\envs\150BLLM\Lib\site-packages\torch\lib" "/LIBPATH:C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.9\lib\x64" "/LIBPATH:C:\Users\Shashank Murthy\.conda\envs\150BLLM\libs" "/LIBPATH:C:\Users\Shashank Murthy\.conda\envs\150BLLM" "/LIBPATH:C:\Users\Shashank Murthy\.conda\envs\150BLLM\PCbuild\amd64" "/LIBPATH:C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.44.35207\lib\x64" "/LIBPATH:C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\lib\um\x64" "/LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.26100.0\ucrt\x64" "/LIBPATH:C:\Program Files (x86)\Windows Kits\10\\lib\10.0.26100.0\\um\x64" c10.lib torch.lib torch_cpu.lib torch_python.lib cudart.lib c10_cuda.lib torch_cuda.lib /EXPORT:PyInit_cuda C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\pybind.obj C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\sm100\prefill\dense\fmha_cutlass_fwd_sm100.obj C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\smxx\get_mla_metadata.obj C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\smxx\mla_combine.obj /OUT:build\lib.win-amd64-cpython-312\flash_mla\cuda.cp312-win_amd64.pyd /IMPLIB:C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\cuda.cp312-win_amd64.lib
   Creating library C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\cuda.cp312-win_amd64.lib and object C:\PyCharmProjectsSpaceConflict\150BLLM\external\FlashMLA\build\temp.win-amd64-cpython-312\Release\csrc\cuda.cp312-win_amd64.exp
Generating code
Finished generating code
copying build\lib.win-amd64-cpython-312\flash_mla\cuda.cp312-win_amd64.pyd -> flash_mla
